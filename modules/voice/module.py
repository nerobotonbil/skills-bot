"""
–ú–æ–¥—É–ª—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≥–æ–ª–æ—Å–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
"""
import logging
import os
import tempfile
from typing import List, Optional
from pathlib import Path

from telegram import Update, Voice
from telegram.ext import MessageHandler, ContextTypes, BaseHandler, filters

from modules.base import BaseModule
from config.settings import OPENAI_API_KEY, DATA_DIR

logger = logging.getLogger(__name__)


class VoiceModule(BaseModule):
    """
    –ú–æ–¥—É–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≥–æ–ª–æ—Å–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π.
    –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≥–æ–ª–æ—Å –≤ —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é OpenAI Whisper.
    –ü–æ—Å–ª–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ –ø–µ—Ä–µ–¥–∞—ë—Ç —Ç–µ–∫—Å—Ç AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—É –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.
    """
    
    def __init__(self):
        super().__init__(
            name="voice",
            description="–û–±—Ä–∞–±–æ—Ç–∫–∞ –≥–æ–ª–æ—Å–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Ç–µ–∫—Å—Ç"
        )
        self._voice_dir = DATA_DIR / "voice"
        self._voice_dir.mkdir(parents=True, exist_ok=True)
        
        # –°—Å—ã–ª–∫–∞ –Ω–∞ AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –º–æ–¥—É–ª—å (—É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ)
        self._ai_assistant = None
    
    def get_handlers(self) -> List[BaseHandler]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏"""
        return [
            MessageHandler(filters.VOICE, self.handle_voice_message),
        ]
    
    def set_ai_assistant(self, ai_assistant):
        """
        –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å—Å—ã–ª–∫—É –Ω–∞ AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.
        """
        self._ai_assistant = ai_assistant
        logger.info("Voice module connected to AI Assistant")
    
    async def handle_voice_message(
        self,
        update: Update,
        context: ContextTypes.DEFAULT_TYPE
    ) -> None:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ"""
        voice = update.message.voice
        
        await update.message.reply_text("üé§ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ...")
        
        try:
            # –°–∫–∞—á–∏–≤–∞–µ–º –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
            voice_file = await context.bot.get_file(voice.file_id)
            
            # –°–æ–∑–¥–∞—ë–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            with tempfile.NamedTemporaryFile(
                suffix=".ogg",
                dir=self._voice_dir,
                delete=False
            ) as tmp_file:
                voice_path = tmp_file.name
            
            # –°–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª
            await voice_file.download_to_drive(voice_path)
            logger.info(f"Voice file downloaded: {voice_path}")
            
            # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º
            text = await self.transcribe_audio(voice_path)
            
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            try:
                os.unlink(voice_path)
            except:
                pass
            
            if text:
                # –ï—Å–ª–∏ –µ—Å—Ç—å AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –ø–µ—Ä–µ–¥–∞—ë–º –µ–º—É —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
                if self._ai_assistant:
                    await self._ai_assistant.process_voice_text(update, context, text)
                else:
                    # –ò–Ω–∞—á–µ –ø—Ä–æ—Å—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç
                    await update.message.reply_text(
                        f"üìù –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:\n\n{text}"
                    )
            else:
                await update.message.reply_text(
                    "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å. –ü–æ–ø—Ä–æ–±—É–π –µ—â—ë —Ä–∞–∑."
                )
                
        except Exception as e:
            logger.error(f"Error processing voice message: {e}")
            await update.message.reply_text(
                f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}"
            )
    
    async def transcribe_audio(self, file_path: str) -> Optional[str]:
        """
        –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ —Ñ–∞–π–ª –≤ —Ç–µ–∫—Å—Ç.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç OpenAI Whisper API.
        """
        if OPENAI_API_KEY:
            text = await self._transcribe_openai(file_path)
            if text:
                return text
        
        # Fallback –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç
        text = await self._transcribe_local(file_path)
        if text:
            return text
        
        return None
    
    async def _transcribe_local(self, file_path: str) -> Optional[str]:
        """–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç —Å –ø–æ–º–æ—â—å—é –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞"""
        import subprocess
        
        try:
            result = subprocess.run(
                ["manus-speech-to-text", file_path],
                capture_output=True,
                text=True,
                timeout=60
            )
            
            if result.returncode == 0:
                text = result.stdout.strip()
                logger.info(f"Local transcription successful: {len(text)} chars")
                return text
            else:
                logger.warning(f"Local transcription failed: {result.stderr}")
                return None
                
        except FileNotFoundError:
            logger.warning("manus-speech-to-text not found")
            return None
        except subprocess.TimeoutExpired:
            logger.warning("Local transcription timed out")
            return None
        except Exception as e:
            logger.error(f"Local transcription error: {e}")
            return None
    
    async def _transcribe_openai(self, file_path: str) -> Optional[str]:
        """–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç —Å –ø–æ–º–æ—â—å—é OpenAI Whisper API"""
        try:
            from openai import OpenAI
            
            client = OpenAI(api_key=OPENAI_API_KEY)
            
            with open(file_path, "rb") as audio_file:
                transcript = client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file,
                    language="ru"  # –†—É—Å—Å–∫–∏–π —è–∑—ã–∫
                )
            
            text = transcript.text.strip()
            logger.info(f"OpenAI transcription successful: {len(text)} chars")
            return text
            
        except ImportError:
            logger.warning("OpenAI package not installed")
            return None
        except Exception as e:
            logger.error(f"OpenAI transcription error: {e}")
            return None
    
    def summarize_text(self, text: str, max_length: int = 200) -> str:
        """
        –°–æ–∫—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–æ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã.
        –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è - –æ–±—Ä–µ–∑–∞–µ—Ç –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º.
        """
        if len(text) <= max_length:
            return text
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = text.replace("!", ".").replace("?", ".").split(".")
        
        result = []
        current_length = 0
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            if current_length + len(sentence) + 2 <= max_length:
                result.append(sentence)
                current_length += len(sentence) + 2
            else:
                break
        
        if result:
            return ". ".join(result) + "."
        else:
            return text[:max_length] + "..."


# –≠–∫–∑–µ–º–ø–ª—è—Ä –º–æ–¥—É–ª—è
voice_module = VoiceModule()
